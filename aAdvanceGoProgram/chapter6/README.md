# 分布式系统

## 分布式id生成器
- `Twitter`的`snowflake`算法:
    ![](distributedId/snowflake.png)
    首先确定我们的数值是64位,int64类型,被划分为四部分,不含开头的第一个bit,因为这个bit是符号位.用41位来表示收到请求时的时间戳,
    单位为毫秒,然后五位来表示数据中心的id,然后再五位来表示机器的实例id,最后是12位的循环自增id（到达1111,1111,1111后会归0）.
    
    这样的机制可以支持我们在同一台机器上,同一毫秒内产生2 ^ 12 = 4096条消息.一秒共409.6万条消息.从值域上来讲完全够用了.
    
    数据中心加上实例id共有10位,可以支持我们每数据中心部署32台机器,所有数据中心共1024台实例.
    
    表示timestamp的41位,可以支持我们使用69年.当然,我们的时间毫秒计数不会真的从1970年开始记,那样我们的系统跑到2039/9/7 23:47:35就不能用了,
    所以这里的timestamp只是相对于某个时间的增量,比如我们的系统上线是2018-08-01,那么我们可以把这个timestamp当作是从
    2018-08-01 00:00:00.000的偏移量.
- `Sony`的`sonyflake`算法: 算法思路与`snowflake`差不多, 只不过位分配不同
    ![](distributedId/snoyflake.png)
    这里的时间只用了39个bit, 但时间的单位变成了10ms, 所以理论上比41位表示的时间还要久(174年).
    没10ms生成256个, 一秒大概2.56万个id, 某些场景可能不够用
    
## 分布式锁
几种加锁的方式:
- 基于Redis的setnx: 如果锁在任何恶劣条件下都不允许数据丢失, 则不应使用此锁
- 基于ZooKeeper
- 基于etcd

## 延时任务系统
延时任务的解决思路:
- 实现一套类似于crontab的分布式定时任务管理系统
- 实现一个支持定时发送消息的消息队列

## 分布式搜索引擎
Elasticsearch, [代码示例](elasticsearch/elasticsearch.go)

### 异构数据同步
在实际应用中,我们很少直接向搜索引擎中写入数据.更为常见的方式是,将MySQL或其它关系型数据中的数据同步到搜索引擎中.
而搜索引擎的使用方只能对数据进行查询,无法进行修改和删除.

常见同步方案:
- 通过时间戳进行增量数据同步
- 通过binlog进行数据同步: 使用较多的是阿里开源的`Canal`, 来进行binlog的解析与同步.
    - canal伪装成MYSQL的从库, 解析好格式的binlog, 再以更容易解析的格式(例如json)发送到消息队列.
    
## 负载均衡

常见负载均衡思路, [代码示例](loadBalance/shuffle.go), [算法比较](loadBalance/shuffleCompare.go): 
- 按顺序挑选
- 随机挑选
- 根据某种权重, 对下游节点进行排序, 选择权重最大或最小的那个

负载均衡策略:
- 基于洗牌算法的负载均衡

## 分布式配置管理
- 配置版本管理: 支持配置按照版本回滚
- 客户端容错: 为应对配置中心宕机的情况, 最好在磁盘本地持久化配置信息. 以供不时之需.

## 分布式爬虫
- [基于colly的单机爬虫](worm/singleWorm.go)
- nats是Go实现的一个高性能分布式消息队列, 适用于高并发高吞吐量的消息分发场景